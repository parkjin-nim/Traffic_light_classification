{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinpark/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jinpark/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jinpark/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jinpark/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jinpark/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jinpark/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/jinpark/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Note that tf ver. is 1.4\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageColor\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(batch_size, features, labels):\n",
    "    output_batches = []\n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 150 150\n",
      "500 150 150\n",
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "RED = 0\n",
    "YELLOW = 1\n",
    "GREEN = 2\n",
    "\n",
    "# n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "# n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "n_input = 2352  # 784*3a\n",
    "n_classes = 3  \n",
    "\n",
    "\n",
    "directories = [\n",
    "    \"traffic_light_images_sim/green/\",\n",
    "    \"traffic_light_images_sim/red/\",\n",
    "    \"traffic_light_images_sim/yellow/\"]\n",
    "\n",
    "folders = [\n",
    "    \"train/\",\n",
    "    \"valid/\",\n",
    "    \"test/\"]\n",
    "\n",
    "dic = {\"traffic_light_images_sim/green/\":2,\n",
    "        \"traffic_light_images_sim/red/\":0,\n",
    "       \"traffic_light_images_sim/yellow/\":1}\n",
    "\n",
    "\n",
    "train_features,valid_features,test_features = [],[],[]\n",
    "train_labels,valid_labels,test_labels = [],[],[]\n",
    "\n",
    "\n",
    "for d in directories:\n",
    "    path  = os.getcwd()\n",
    "    path = os.path.join(path, d)\n",
    "    \n",
    "    for fd in folders:\n",
    "        new_path = os.path.join(path, fd)\n",
    "        files = os.listdir(new_path)\n",
    "        \n",
    "        for f in files:\n",
    "            if f.endswith(\"png\"):\n",
    "                sim_img = Image.open(new_path+f)\n",
    "                rsz_image = cv2.resize(np.array(sim_img), (28,28))\n",
    "                rsz_image = np.array(rsz_image).flatten()\n",
    "                \n",
    "                temp = np.zeros(3)\n",
    "                if fd == \"train/\":\n",
    "                    train_features.append(rsz_image)\n",
    "                    temp[dic[d]]=1.\n",
    "                    train_labels.append(temp.copy())\n",
    "                elif fd == \"valid/\":\n",
    "                    valid_features.append(rsz_image)\n",
    "                    temp[dic[d]]=1.\n",
    "                    valid_labels.append(temp.copy())\n",
    "                elif fd == \"test/\":\n",
    "                    test_features.append(rsz_image)\n",
    "                    temp[dic[d]]=1.\n",
    "                    test_labels.append(temp.copy())\n",
    "\n",
    "print(len(train_features), len(valid_features), len(test_features))\n",
    "print(len(train_labels), len(valid_labels), len(test_labels))\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input], name=\"ft\")\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias, name=\"op\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0    - Cost: 0.0      Valid Accuracy: 0.413\n",
      "Epoch: 1    - Cost: 0.0      Valid Accuracy: 0.52 \n",
      "Epoch: 2    - Cost: 0.0      Valid Accuracy: 0.5  \n",
      "Epoch: 3    - Cost: 0.0      Valid Accuracy: 0.88 \n",
      "Epoch: 4    - Cost: 0.0      Valid Accuracy: 0.927\n",
      "Epoch: 5    - Cost: 0.0      Valid Accuracy: 0.967\n",
      "Epoch: 6    - Cost: 0.0      Valid Accuracy: 0.68 \n",
      "Epoch: 7    - Cost: 0.0      Valid Accuracy: 0.94 \n",
      "Epoch: 8    - Cost: 0.0      Valid Accuracy: 0.94 \n",
      "Epoch: 9    - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 10   - Cost: 0.0      Valid Accuracy: 0.927\n",
      "Epoch: 11   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 12   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 13   - Cost: 0.0      Valid Accuracy: 0.913\n",
      "Epoch: 14   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 15   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 16   - Cost: 0.0      Valid Accuracy: 0.94 \n",
      "Epoch: 17   - Cost: 0.0      Valid Accuracy: 0.927\n",
      "Epoch: 18   - Cost: 0.0      Valid Accuracy: 0.98 \n",
      "Epoch: 19   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 20   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 21   - Cost: 0.0      Valid Accuracy: 0.953\n",
      "Epoch: 22   - Cost: 0.0      Valid Accuracy: 0.927\n",
      "Epoch: 23   - Cost: 0.0      Valid Accuracy: 0.953\n",
      "Epoch: 24   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 25   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 26   - Cost: 0.0      Valid Accuracy: 0.94 \n",
      "Epoch: 27   - Cost: 0.0      Valid Accuracy: 0.947\n",
      "Epoch: 28   - Cost: 0.0      Valid Accuracy: 0.953\n",
      "Epoch: 29   - Cost: 0.0      Valid Accuracy: 0.96 \n",
      "Epoch: 30   - Cost: 0.0      Valid Accuracy: 0.96 \n",
      "Epoch: 31   - Cost: 0.0      Valid Accuracy: 0.927\n",
      "Epoch: 32   - Cost: 0.0      Valid Accuracy: 0.94 \n",
      "Epoch: 33   - Cost: 0.0      Valid Accuracy: 0.94 \n",
      "Epoch: 34   - Cost: 0.0      Valid Accuracy: 0.947\n",
      "Epoch: 35   - Cost: 0.0      Valid Accuracy: 0.94 \n",
      "Epoch: 36   - Cost: 0.0      Valid Accuracy: 0.947\n",
      "Epoch: 37   - Cost: 0.0      Valid Accuracy: 0.947\n",
      "Epoch: 38   - Cost: 0.0      Valid Accuracy: 0.947\n",
      "Epoch: 39   - Cost: 0.0      Valid Accuracy: 0.947\n",
      "Epoch: 40   - Cost: 0.0      Valid Accuracy: 0.96 \n",
      "Epoch: 41   - Cost: 0.0      Valid Accuracy: 0.967\n",
      "Epoch: 42   - Cost: 0.0      Valid Accuracy: 0.92 \n",
      "Epoch: 43   - Cost: 0.0      Valid Accuracy: 0.92 \n",
      "Epoch: 44   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 45   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 46   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 47   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 48   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Epoch: 49   - Cost: 0.0      Valid Accuracy: 0.933\n",
      "Test Accuracy: 0.9066666960716248\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "    \n",
    "    # Save my tiny model\n",
    "    saver.save(sess, \"./model/my_tiny_model\")\n",
    "    \n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/my_tiny_model\n",
      "(1, 2352)\n",
      "[ 28122.121     -77.7169 -33981.676 ]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    #First let's load meta graph and restore weights\n",
    "    saved = tf.train.import_meta_graph('./model/my_tiny_model.meta')\n",
    "    saved.restore(sess,tf.train.latest_checkpoint('./model'))\n",
    "\n",
    "\n",
    "    # Now, let's access and create placeholders variables and\n",
    "    # create feed-dict to feed new data\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    w1 = graph.get_tensor_by_name(\"ft:0\")\n",
    "\n",
    "    cv2_mage = Image.open(\"./traffic_light_images_sim/red.jpg\")\n",
    "    image = cv2.resize(np.array(cv2_mage), (28,28))\n",
    "    image = np.array(image).flatten()     \n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    print(image.shape)\n",
    "    feed_dict ={w1:image}\n",
    "\n",
    "    #Now, access the op that you want to run. \n",
    "    op_to_restore = graph.get_tensor_by_name(\"op:0\")\n",
    "\n",
    "\n",
    "\n",
    "    print(np.squeeze(sess.run(op_to_restore,feed_dict)))\n",
    "    #This will print 60 which is calculated \n",
    "    #using new values of w1 and w2 and saved value of b1. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.train.write_graph(sess.graph_def,'.','model/my_tiny_graph.pb', as_text=False )\n",
    "# tf.train.write_graph(sess.graph_def,'.','model/my_tiny_graph.pbtxt', as_text=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = './model/my_tiny_graph.pb'\n",
    "\n",
    "# graph = tf.Graph()\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# #config.gpu_options.allow_growth = True\n",
    "\n",
    "# with graph.as_default():\n",
    "#     graph_def = tf.GraphDef()\n",
    "\n",
    "#     with tf.gfile.GFile(model_path, 'rb') as graph_file:\n",
    "#         read_graph_file = graph_file.read()\n",
    "\n",
    "#         graph_def.ParseFromString(read_graph_file)\n",
    "#         tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "#     sess = tf.Session(graph=graph, config=config)\n",
    "\n",
    "#     restored_op = sess.graph.get_tensor_by_name('op:0')\n",
    "#     restored_ft = sess.graph.get_tensor_by_name('ft:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cv2_image = Image.open(\"./ex/green.jpg\")\n",
    "\n",
    "# image = cv2.resize(np.array(cv2_image), (28,28))\n",
    "# image = np.array(image).flatten()                \n",
    "\n",
    "\n",
    "\n",
    "# with graph.as_default():\n",
    "#     cv2_image_expanded = np.expand_dims(image, axis=0)\n",
    "\n",
    "#     print(sess.run([restored_op], feed_dict={restored_ft: cv2_image_expanded}))\n",
    "# #     classes = np.squeeze(classes)\n",
    "# #     scores = np.squeeze(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = './model/my_tiny_graph.pb'\n",
    "# confidence_threshold = 0.8\n",
    "\n",
    "\n",
    "# graph = tf.Graph()\n",
    "# config = tf.ConfigProto()\n",
    "# # # config.gpu_options.allow_growth = True\n",
    "\n",
    "# with graph.as_default():\n",
    "#     graph_def = tf.GraphDef()\n",
    "\n",
    "#     with tf.gfile.GFile(model_path, 'rb') as graph_file:\n",
    "#         read_graph_file = graph_file.read()\n",
    "\n",
    "#         graph_def.ParseFromString(read_graph_file)\n",
    "#         tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "#     sess = tf.Session(graph=graph, config=config)\n",
    "    \n",
    "#     #sess = tf.Session(graph=graph)\n",
    "\n",
    "#     #     image_tensor = graph.get_tensor_by_name('image_tensor:0')\n",
    "#     #     num_detections = graph.get_tensor_by_name('num_detections:0')\n",
    "#     #     # For each detection, it's corresponding bounding box, class and score:\n",
    "#     #     # self.boxes = self.graph.get_tensor_by_name('detection_boxes:0')\n",
    "#     restored_op = sess.graph.get_tensor_by_name('op:0')\n",
    "#     #     scores =self.graph.get_tensor_by_name('detection_scores:0')\n",
    "#     restored_ft = sess.graph.get_tensor_by_name('ft:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2_image = Image.open(\"./ex/green.jpg\")\n",
    "\n",
    "# image = cv2.resize(np.array(cv2_image), (28,28))\n",
    "# image = np.array(image).flatten()                \n",
    "# image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# with sess.graph.as_default():\n",
    "#     print(sess.run(restored_op,feed_dict={restored_ft: image}))\n",
    "\n",
    "# #    classes = np.squeeze(classes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
